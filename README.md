# ğŸš€ Jusper - AI-Powered E-commerce Scraper

**Jusper** (ex Mr. Anto Scraper) Ã¨ un sistema avanzato di web scraping per e-commerce con analisi AI integrata, dashboard in tempo reale e **il nuovo sistema AI-Assisted per scraping guidato**.

## ğŸ¤– **NUOVO**: AI-Assisted Scraper

**Sistema rivoluzionario** che utilizza l'AI per guidarti nel processo di scraping!

### ğŸ¯ Come Funziona:
1. **ğŸ“ Inserisci URL** â†’ L'AI analizza automaticamente la pagina
2. **ğŸ§  Analisi Intelligente** â†’ "Cosa vuoi scrapare? Ho trovato questi elementi..."
3. **âš™ï¸ Schema Personalizzato** â†’ Definisci cosa estrarre in formato JSON
4. **âœ… Validazione & Estrazione** â†’ Risultati automatici con export!

### ğŸŒŸ Caratteristiche AI-Assisted:
- **Analisi AI automatica** del contenuto delle pagine web
- **Suggerimenti intelligenti** su cosa si puÃ² estrarre
- **Validazione schema** in tempo reale
- **Interfaccia step-by-step** intuitiva
- **Export automatico** in JSON/CSV

### ğŸš€ Accesso Rapido:
- **Interfaccia AI-Assisted**: Apri `Frontend/ai-assisted-scraper.html`
- **Test del Sistema**: `python Backend/test_ai_assisted.py`

## âœ¨ Caratteristiche Principali

- ğŸ¤– **Analisi AI Avanzata** - Supporto per Gemini, GPT-4 e analisi locale
- ğŸ”— **LLM Scraper Integration** - Integrazione con [llm-scraper](https://github.com/mishushakov/llm-scraper) per scraping avanzato
- ğŸ“Š **Dashboard in Tempo Reale** - Statistiche live e metriche di performance  
- ğŸ¯ **Scraping Intelligente** - Estrazione automatica di prodotti e prezzi
- ğŸ’¾ **Persistenza Dati** - Salvataggio automatico in database SQLite
- ğŸ”„ **ModalitÃ  Batch** - Elaborazione di URL multipli
- ğŸ“± **UI Moderna** - Interfaccia Vue.js responsive e intuitiva
- ğŸš€ **Architettura Asincrona** - Performance ottimizzate con async/await
- âš–ï¸ **Sistema di Confronto Avanzato** - Confronto intelligente tra prodotti di siti diversi
- ğŸŒ **Filtri Dinamici** - Selezione multipla di domini per confronti mirati

## ğŸ› ï¸ Installazione Rapida

### 1. Clona il Repository
```bash
git clone https://github.com/tuo-username/jusper.git
cd jusper
```

### 2. Installa le Dipendenze
```bash
pip install -r requirements.txt
```

### 3. Configura le Variabili d'Ambiente
```bash
# Copia il file di esempio
cp Backend/env.local Backend/env.local.backup

# Modifica Backend/env.local con le tue API keys
```

### 4. Avvia il Server
```bash
python start.py
```

Apri http://localhost:8000 nel browser! ğŸ‰

## ğŸš€ Deploy su Railway (Hosting Gratuito)

### Deploy con Un Click
[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/jusper)

### Deploy Manuale
1. Crea account su [Railway.app](https://railway.app)
2. Connetti il tuo repository GitHub
3. Configura le variabili d'ambiente:
   - `AI_PROVIDER=local` (o gemini/openai)
   - `GEMINI_API_KEY=tua_key` (se usi Gemini)
   - `OPENAI_API_KEY=tua_key` (se usi OpenAI)
4. Deploy automatico! âœ¨

## âš™ï¸ Configurazione AI

### ModalitÃ  Locale (Gratuita)
```bash
AI_PROVIDER=local
```
- âœ… Nessuna API key richiesta
- âœ… Completamente gratuito
- âœ… Privacy totale

### ModalitÃ  Gemini (Consigliata)
```bash
AI_PROVIDER=gemini
GEMINI_API_KEY=your_api_key
```
- ğŸš€ Analisi piÃ¹ precisa
- ğŸ’° Tier gratuito generoso
- ğŸ”— [Ottieni API Key](https://makersuite.google.com/app/apikey)

### ModalitÃ  OpenAI
```bash
AI_PROVIDER=openai
OPENAI_API_KEY=your_api_key
```
- ğŸ¯ Massima precisione
- ğŸ’µ A pagamento
- ğŸ”— [Ottieni API Key](https://platform.openai.com/api-keys)

## ğŸ“Š Dashboard Features

- **Statistiche Giornaliere** - Prodotti scansionati, accuratezza AI
- **AttivitÃ  Recenti** - Log in tempo reale delle operazioni
- **Categorie Prodotti** - Classificazione automatica
- **Metriche Performance** - Tempi di risposta e ottimizzazioni
- **Reset Giornaliero** - Statistiche che si resettano ogni 24h

## ğŸ”§ Utilizzo

### Scraping Singolo URL
1. Inserisci l'URL del sito e-commerce
2. Seleziona il provider AI
3. Clicca "Avvia Scraping"
4. Visualizza i risultati nella dashboard

### Scraping Multiplo
1. Clicca "ModalitÃ  Multipla"
2. Inserisci gli URL (uno per riga)
3. Configura le opzioni avanzate
4. Avvia l'elaborazione batch

### Confronto Prodotti Avanzato
1. **ModalitÃ  "Tutti i Prodotti"**: Confronta automaticamente tutti i prodotti di tutti i siti
2. **ModalitÃ  "Domini Specifici"**: Seleziona 2+ siti da confrontare per analisi mirata
3. **Risultati Intelligenti**: Trova prodotti simili con prezzi diversi tra siti diversi
4. **Filtri Dinamici**: Selezione multipla di domini con conteggio prodotti in tempo reale

## ğŸ”— LLM Scraper Integration

**Nuova integrazione** con [LLM Scraper](https://github.com/mishushakov/llm-scraper) per funzionalitÃ  avanzate di scraping!

### ğŸš€ Caratteristiche LLM Scraper:
- **Schema-based Extraction** - Estrazione basata su schemi Zod/JSON
- **Multiple LLM Support** - OpenAI, Anthropic, Google AI
- **Streaming Support** - Streaming di dati in tempo reale
- **Code Generation** - Generazione automatica di script Playwright
- **Multi-modal Support** - Supporto per immagini e screenshot

### ğŸ“¦ Installazione Automatica:
```bash
cd Backend
python install_llm_scraper.py
```

### ğŸ“¦ Installazione Manuale:
```bash
cd Backend
npm install
npx playwright install chromium
python test_llm_scraper.py
```

### ğŸ”§ Utilizzo Rapido:
```python
from llm_scraper_bridge import scrape_with_llm_scraper

result = await scrape_with_llm_scraper(
    url="https://example.com",
    schema={"type": "object", "properties": {"products": {"type": "array"}}},
    format_type="html"
)
```

ğŸ“– **Documentazione completa**: `Backend/LLM_SCRAPER_README.md`

## ğŸ—ï¸ Struttura del Progetto

```
jusper/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ mr_anto_scraper.py      # API principale FastAPI
â”‚   â”œâ”€â”€ html_analyzer.py        # Analizzatore HTML/AI asincrono
â”‚   â”œâ”€â”€ ai_content_analyzer.py  # Analizzatore contenuto AI
â”‚   â”œâ”€â”€ ai_product_comparator.py # Confronto intelligente prodotti con AI
â”‚   â”œâ”€â”€ historical_products_db.py # Database prodotti storici
â”‚   â”œâ”€â”€ llm_scraper_bridge.py   # Bridge per LLM Scraper
â”‚   â”œâ”€â”€ test_llm_scraper.py     # Test LLM Scraper
â”‚   â”œâ”€â”€ package.json            # Dipendenze Node.js
â”‚   â”œâ”€â”€ selector_database.py    # Gestione selettori CSS
â”‚   â”œâ”€â”€ unified_scraper.py      # Scraper unificato
â”‚   â”œâ”€â”€ progressive_scraper.py  # Scraper progressivo
â”‚   â”œâ”€â”€ price_monitor.py        # Monitoraggio prezzi
â”‚   â”œâ”€â”€ price_extractor.py      # Estrazione prezzi
â”‚   â”œâ”€â”€ google_price_finder.py  # Ricerca prezzi Google
â”‚   â”œâ”€â”€ google_vision_finder.py # Ricerca con Google Vision
â”‚   â”œâ”€â”€ playwright_selector_finder.py # Trova selettori con Playwright
â”‚   â”œâ”€â”€ scraping_logic.py       # Logica di scraping
â”‚   â”œâ”€â”€ utils.py                # UtilitÃ  generali
â”‚   â”œâ”€â”€ cache_manager.py        # Gestione cache
â”‚   â”œâ”€â”€ price_scheduler.py      # Scheduler prezzi
â”‚   â”œâ”€â”€ env.local               # Configurazione ambiente
â”‚   â””â”€â”€ database/               # Database SQLite
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ index.html             # UI Vue.js con sistema confronto avanzato
â”‚   â”œâ”€â”€ css/styles.css         # Stili CSS personalizzati
â”‚   â””â”€â”€ js/                    # Moduli JavaScript modulari
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cache/                 # Cache dati
â”‚   â”œâ”€â”€ database/              # Database selettori
â”‚   â””â”€â”€ api_extracts/          # Estrazioni API salvate
â”œâ”€â”€ docs/                      # Documentazione
â”œâ”€â”€ start.py                   # Script di avvio
â”œâ”€â”€ requirements.txt           # Dipendenze Python
â”œâ”€â”€ Procfile                  # Configurazione deploy
â””â”€â”€ railway.json             # Configurazione Railway
```

## ğŸ› Risoluzione Problemi

### Errore "Browser has been closed"
- Assicurati che Playwright sia installato: `playwright install`
- Riavvia il server dopo cambi di configurazione

### Problemi con API Keys
- Verifica che le chiavi siano corrette in `Backend/env.local`
- Controlla i limiti di quota delle API

### Port giÃ  in uso
- Cambia porta in `start.py` o usa: `python start.py --port 8001`

## ğŸ¤ Contribuire

1. Fork del repository
2. Crea un branch feature (`git checkout -b feature/nuova-feature`)
3. Commit delle modifiche (`git commit -am 'Aggiungi nuova feature'`)
4. Push del branch (`git push origin feature/nuova-feature`)
5. Crea una Pull Request

## ğŸ“ Licenza

Questo progetto Ã¨ distribuito sotto licenza MIT. Vedi `LICENSE` per maggiori dettagli.

## ğŸ™ Crediti

Sviluppato con â¤ï¸ per la community del web scraping.

---

**â­ Se ti piace il progetto, lascia una stella su GitHub!** 